"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.BuilderSubgraph = exports.BuilderSubgraphState = void 0;
const prompts_1 = require("@langchain/core/prompts");
const langgraph_1 = require("@langchain/langgraph");
const errors_1 = require("../errors");
const subgraph_interface_1 = require("./subgraph-interface");
const add_node_tool_1 = require("../tools/add-node.tool");
const connect_nodes_tool_1 = require("../tools/connect-nodes.tool");
const remove_connection_tool_1 = require("../tools/remove-connection.tool");
const remove_node_tool_1 = require("../tools/remove-node.tool");
const validate_structure_tool_1 = require("../tools/validate-structure.tool");
const coordination_1 = require("../types/coordination");
const cache_control_1 = require("../utils/cache-control");
const context_builders_1 = require("../utils/context-builders");
const operations_processor_1 = require("../utils/operations-processor");
const subgraph_helpers_1 = require("../utils/subgraph-helpers");
const BUILDER_PROMPT = `You are a Builder Agent specialized in constructing n8n workflows.

MANDATORY EXECUTION SEQUENCE:
You MUST follow these steps IN ORDER. Do not skip any step.

STEP 1: CREATE NODES
- Call add_nodes for EVERY node needed based on discovery results
- Create multiple nodes in PARALLEL for efficiency
- Do NOT respond with text - START BUILDING immediately

STEP 2: CONNECT NODES
- Call connect_nodes for ALL required connections
- Connect multiple node pairs in PARALLEL

STEP 3: VALIDATE (REQUIRED)
- After ALL nodes and connections are created, call validate_structure
- This step is MANDATORY - you cannot finish without it
- If validation finds issues (missing trigger, invalid connections), fix them and validate again

STEP 4: RESPOND TO USER
- Only after validation passes, provide your brief summary

⚠️ NEVER respond to the user without calling validate_structure first ⚠️

NODE CREATION:
Each add_nodes call creates ONE node. You must provide:
- nodeType: The exact type from discovery (e.g., "n8n-nodes-base.httpRequest")
- name: Descriptive name (e.g., "Fetch Weather Data")
- connectionParametersReasoning: Explain your thinking about connection parameters
- connectionParameters: Parameters that affect connections (or {{}} if none needed)

<workflow_configuration_node>
Always include a Workflow Configuration node at the start of every workflow.

The Workflow Configuration node (n8n-nodes-base.set) should be placed immediately after the trigger node and before all other processing nodes.

Placement rules:
- Add between trigger and first processing node
- Connect: Trigger → Workflow Configuration → First processing node
- Name it "Workflow Configuration"
</workflow_configuration_node>

<data_parsing_strategy>
For AI-generated structured data, prefer Structured Output Parser nodes over Code nodes.
For binary file data, use Extract From File node to extract content from files before processing.
Use Code nodes only for custom business logic beyond parsing.
</data_parsing_strategy>

<proactive_design>
Anticipate workflow needs:
- IF nodes for conditional logic when multiple outcomes exist
- Set nodes for data transformation between incompatible formats
- Schedule Triggers for recurring tasks
- Error handling for external service calls

NEVER use Split In Batches nodes.
</proactive_design>

<node_defaults_warning>
CRITICAL: NEVER RELY ON DEFAULT PARAMETER VALUES FOR CONNECTIONS

Default values often hide connection inputs/outputs. You MUST explicitly configure parameters that affect connections:
- Vector Store: Mode parameter affects available connections - always set explicitly (e.g., mode: "insert", "retrieve", "retrieve-as-tool")
- AI Agent: hasOutputParser default may not match your workflow needs
- Document Loader: textSplittingMode affects whether it accepts a text splitter input

ALWAYS check node details and set connectionParameters explicitly.
</node_defaults_warning>

CONNECTION PARAMETERS EXAMPLES:
- Static nodes (HTTP Request, Set, Code): reasoning="Static inputs/outputs", parameters={{}}
- AI Agent with parser: reasoning="hasOutputParser creates additional input", parameters={{ hasOutputParser: true }}
- Vector Store insert: reasoning="Insert mode requires document input", parameters={{ mode: "insert" }}
- Document Loader custom: reasoning="Custom mode enables text splitter input", parameters={{ textSplittingMode: "custom" }}

<node_connections_understanding>
n8n connections flow from SOURCE (output) to TARGET (input).

Regular data flow: Source node output → Target node input
Example: HTTP Request → Set (HTTP Request is source, Set is target)

AI sub-nodes PROVIDE capabilities, making them the SOURCE:
- OpenAI Chat Model → AI Agent [ai_languageModel]
- Calculator Tool → AI Agent [ai_tool]
- Window Buffer Memory → AI Agent [ai_memory]
- Token Splitter → Default Data Loader [ai_textSplitter]
- Default Data Loader → Vector Store [ai_document]
- Embeddings OpenAI → Vector Store [ai_embedding]
</node_connections_understanding>

<agent_node_distinction>
Distinguish between two different agent node types:

1. **AI Agent** (@n8n/n8n-nodes-langchain.agent)
   - Main workflow node that orchestrates AI tasks
   - Use for: Primary AI logic, chatbots, autonomous workflows

2. **AI Agent Tool** (@n8n/n8n-nodes-langchain.agentTool)
   - Sub-node that acts as a tool for another AI Agent
   - Use for: Multi-agent systems where one agent calls another

Default assumption: When discovery results include "agent", use AI Agent
unless explicitly specified as "agent tool" or "sub-agent".
</agent_node_distinction>

<rag_workflow_pattern>
For RAG (Retrieval-Augmented Generation) workflows:

Main data flow:
- Data source (e.g., HTTP Request) → Vector Store [main connection]

AI capability connections:
- Document Loader → Vector Store [ai_document]
- Embeddings → Vector Store [ai_embedding]
- Text Splitter → Document Loader [ai_textSplitter]

Common mistake to avoid:
- NEVER connect Document Loader to main data outputs
- Document Loader is an AI sub-node that gives Vector Store document processing capability
</rag_workflow_pattern>

<connection_type_examples>
**Main Connections** (regular data flow):
- Trigger → HTTP Request → Set → Email

**AI Language Model Connections** (ai_languageModel):
- OpenAI Chat Model → AI Agent

**AI Tool Connections** (ai_tool):
- Calculator Tool → AI Agent
- AI Agent Tool → AI Agent (for multi-agent systems)

**AI Document Connections** (ai_document):
- Document Loader → Vector Store

**AI Embedding Connections** (ai_embedding):
- OpenAI Embeddings → Vector Store

**AI Text Splitter Connections** (ai_textSplitter):
- Token Text Splitter → Document Loader

**AI Memory Connections** (ai_memory):
- Window Buffer Memory → AI Agent

**AI Vector Store in retrieve-as-tool mode** (ai_tool):
- Vector Store → AI Agent
</connection_type_examples>

DO NOT:
- Respond before calling validate_structure
- Skip validation even if you think structure is correct
- Add commentary between tool calls - execute tools silently
- Configure node parameters (that's the Configurator Agent's job)
- Search for nodes (that's the Discovery Agent's job)
- Make assumptions about node types - use exactly what Discovery found

RESPONSE FORMAT (only after validation):
Provide ONE brief text message summarizing:
- What nodes were added
- How they're connected

Example: "Created 4 nodes: Trigger → Weather → Image Generation → Email"`;
exports.BuilderSubgraphState = langgraph_1.Annotation.Root({
    workflowJSON: (0, langgraph_1.Annotation)({
        reducer: (x, y) => y ?? x,
        default: () => ({ nodes: [], connections: {}, name: '' }),
    }),
    userRequest: (0, langgraph_1.Annotation)({
        reducer: (x, y) => y ?? x,
        default: () => '',
    }),
    workflowContext: (0, langgraph_1.Annotation)({
        reducer: (x, y) => y ?? x,
    }),
    discoveryContext: (0, langgraph_1.Annotation)({
        reducer: (x, y) => y ?? x,
        default: () => null,
    }),
    messages: (0, langgraph_1.Annotation)({
        reducer: (x, y) => x.concat(y),
        default: () => [],
    }),
    workflowOperations: (0, langgraph_1.Annotation)({
        reducer: (x, y) => {
            if (y === null)
                return [];
            if (!y || y.length === 0)
                return x ?? [];
            return [...(x ?? []), ...y];
        },
        default: () => [],
    }),
});
class BuilderSubgraph extends subgraph_interface_1.BaseSubgraph {
    name = 'builder_subgraph';
    description = 'Constructs workflow structure: creating nodes and connections';
    create(config) {
        const tools = [
            (0, add_node_tool_1.createAddNodeTool)(config.parsedNodeTypes),
            (0, connect_nodes_tool_1.createConnectNodesTool)(config.parsedNodeTypes, config.logger),
            (0, remove_node_tool_1.createRemoveNodeTool)(config.logger),
            (0, remove_connection_tool_1.createRemoveConnectionTool)(config.logger),
            (0, validate_structure_tool_1.createValidateStructureTool)(config.parsedNodeTypes),
        ];
        const toolMap = new Map(tools.map((bt) => [bt.tool.name, bt.tool]));
        const systemPrompt = prompts_1.ChatPromptTemplate.fromMessages([
            [
                'system',
                [
                    {
                        type: 'text',
                        text: BUILDER_PROMPT,
                        cache_control: { type: 'ephemeral' },
                    },
                ],
            ],
            ['placeholder', '{messages}'],
        ]);
        if (typeof config.llm.bindTools !== 'function') {
            throw new errors_1.LLMServiceError('LLM does not support tools', {
                llmModel: config.llm._llmType(),
            });
        }
        const agent = systemPrompt.pipe(config.llm.bindTools(tools.map((bt) => bt.tool)));
        const callAgent = async (state) => {
            (0, cache_control_1.applySubgraphCacheMarkers)(state.messages);
            const response = await agent.invoke({
                messages: state.messages,
            });
            return { messages: [response] };
        };
        const shouldContinue = (0, subgraph_helpers_1.createStandardShouldContinue)();
        const subgraph = new langgraph_1.StateGraph(exports.BuilderSubgraphState)
            .addNode('agent', callAgent)
            .addNode('tools', async (state) => await (0, subgraph_helpers_1.executeSubgraphTools)(state, toolMap))
            .addNode('process_operations', operations_processor_1.processOperations)
            .addEdge('__start__', 'agent')
            .addConditionalEdges('agent', shouldContinue)
            .addEdge('tools', 'process_operations')
            .addEdge('process_operations', 'agent');
        return subgraph.compile();
    }
    transformInput(parentState) {
        const userRequest = (0, subgraph_helpers_1.extractUserRequest)(parentState.messages);
        const contextParts = [];
        contextParts.push('=== USER REQUEST ===');
        contextParts.push(userRequest);
        if (parentState.discoveryContext) {
            contextParts.push('=== DISCOVERY CONTEXT ===');
            contextParts.push((0, context_builders_1.buildDiscoveryContextBlock)(parentState.discoveryContext, true));
        }
        contextParts.push('=== CURRENT WORKFLOW ===');
        if (parentState.workflowJSON.nodes.length > 0) {
            contextParts.push((0, context_builders_1.buildWorkflowJsonBlock)(parentState.workflowJSON));
        }
        else {
            contextParts.push('Empty workflow - ready to build');
        }
        const schemaBlock = (0, context_builders_1.buildExecutionSchemaBlock)(parentState.workflowContext);
        if (schemaBlock) {
            contextParts.push('=== AVAILABLE DATA SCHEMA ===');
            contextParts.push(schemaBlock);
        }
        const contextMessage = (0, context_builders_1.createContextMessage)(contextParts);
        return {
            userRequest,
            workflowJSON: parentState.workflowJSON,
            workflowContext: parentState.workflowContext,
            discoveryContext: parentState.discoveryContext,
            messages: [contextMessage],
        };
    }
    transformOutput(subgraphOutput, _parentState) {
        const nodes = subgraphOutput.workflowJSON.nodes;
        const connections = subgraphOutput.workflowJSON.connections;
        const connectionCount = Object.values(connections).flat().length;
        const builderSummary = subgraphOutput.messages
            .slice()
            .reverse()
            .find((m) => m.content &&
            (!('tool_calls' in m) ||
                !m.tool_calls ||
                (m.tool_calls && Array.isArray(m.tool_calls) && m.tool_calls.length === 0)));
        const summaryText = typeof builderSummary?.content === 'string' ? builderSummary.content : undefined;
        const logEntry = {
            phase: 'builder',
            status: 'completed',
            timestamp: Date.now(),
            summary: `Created ${nodes.length} nodes with ${connectionCount} connections`,
            output: summaryText,
            metadata: (0, coordination_1.createBuilderMetadata)({
                nodesCreated: nodes.length,
                connectionsCreated: connectionCount,
                nodeNames: nodes.map((n) => n.name),
            }),
        };
        return {
            workflowJSON: subgraphOutput.workflowJSON,
            workflowOperations: subgraphOutput.workflowOperations ?? [],
            coordinationLog: [logEntry],
        };
    }
}
exports.BuilderSubgraph = BuilderSubgraph;
//# sourceMappingURL=builder.subgraph.js.map