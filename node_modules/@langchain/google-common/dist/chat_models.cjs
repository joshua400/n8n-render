const require_rolldown_runtime = require('./_virtual/rolldown_runtime.cjs');
const require_zod_to_gemini_parameters = require('./utils/zod_to_gemini_parameters.cjs');
const require_gemini = require('./utils/gemini.cjs');
const require_common = require('./utils/common.cjs');
const require_failed_handler = require('./utils/failed_handler.cjs');
const require_connection = require('./connection.cjs');
const require_auth = require('./auth.cjs');
const require_profiles = require('./profiles.cjs');
const __langchain_core_utils_env = require_rolldown_runtime.__toESM(require("@langchain/core/utils/env"));
const __langchain_core_language_models_chat_models = require_rolldown_runtime.__toESM(require("@langchain/core/language_models/chat_models"));
const __langchain_core_outputs = require_rolldown_runtime.__toESM(require("@langchain/core/outputs"));
const __langchain_core_messages = require_rolldown_runtime.__toESM(require("@langchain/core/messages"));
const __langchain_core_runnables = require_rolldown_runtime.__toESM(require("@langchain/core/runnables"));
const __langchain_core_output_parsers_openai_tools = require_rolldown_runtime.__toESM(require("@langchain/core/output_parsers/openai_tools"));
const __langchain_core_utils_stream = require_rolldown_runtime.__toESM(require("@langchain/core/utils/stream"));
const __langchain_core_utils_types = require_rolldown_runtime.__toESM(require("@langchain/core/utils/types"));

//#region src/chat_models.ts
var ChatConnection = class extends require_connection.AbstractGoogleLLMConnection {
	convertSystemMessageToHumanContent;
	constructor(fields, caller, client, streaming) {
		super(fields, caller, client, streaming);
		this.convertSystemMessageToHumanContent = fields?.convertSystemMessageToHumanContent;
	}
	get useSystemInstruction() {
		return typeof this.convertSystemMessageToHumanContent === "boolean" ? !this.convertSystemMessageToHumanContent : this.computeUseSystemInstruction;
	}
	get computeUseSystemInstruction() {
		if (this.modelFamily === "palm") return false;
		else if (this.modelName === "gemini-1.0-pro-001") return false;
		else if (this.modelName.startsWith("gemini-pro-vision")) return false;
		else if (this.modelName.startsWith("gemini-1.0-pro-vision")) return false;
		else if (this.modelName === "gemini-pro" && this.platform === "gai") return false;
		else if (this.modelFamily === "gemma") return false;
		return true;
	}
	computeGoogleSearchToolAdjustmentFromModel() {
		if (this.modelName.startsWith("gemini-1.0")) return "googleSearchRetrieval";
		else if (this.modelName.startsWith("gemini-1.5")) return "googleSearchRetrieval";
		else return "googleSearch";
	}
	computeGoogleSearchToolAdjustment(apiConfig) {
		const adj = apiConfig.googleSearchToolAdjustment;
		if (adj === void 0 || adj === true) return this.computeGoogleSearchToolAdjustmentFromModel();
		else return adj;
	}
	buildGeminiAPI() {
		const apiConfig = this.apiConfig ?? {};
		const googleSearchToolAdjustment = this.computeGoogleSearchToolAdjustment(apiConfig);
		const geminiConfig = {
			useSystemInstruction: this.useSystemInstruction,
			googleSearchToolAdjustment,
			...apiConfig
		};
		return require_gemini.getGeminiAPI(geminiConfig);
	}
	get api() {
		switch (this.apiName) {
			case "google": return this.buildGeminiAPI();
			default: return super.api;
		}
	}
};
/**
* Integration with a Google chat model.
*/
var ChatGoogleBase = class extends __langchain_core_language_models_chat_models.BaseChatModel {
	static lc_name() {
		return "ChatGoogle";
	}
	get lc_secrets() {
		return { authOptions: "GOOGLE_AUTH_OPTIONS" };
	}
	lc_serializable = true;
	model;
	modelName = "gemini-pro";
	temperature;
	maxOutputTokens;
	maxReasoningTokens;
	topP;
	topK;
	seed;
	presencePenalty;
	frequencyPenalty;
	stopSequences = [];
	logprobs;
	topLogprobs = 0;
	safetySettings = [];
	responseModalities;
	convertSystemMessageToHumanContent;
	safetyHandler;
	speechConfig;
	streamUsage = true;
	streaming = false;
	labels;
	connection;
	streamedConnection;
	constructor(fields) {
		super(require_failed_handler.ensureParams(fields));
		require_common.copyAndValidateModelParamsInto(fields, this);
		this.safetyHandler = fields?.safetyHandler ?? new require_gemini.DefaultGeminiSafetyHandler();
		this.streamUsage = fields?.streamUsage ?? this.streamUsage;
		const client = this.buildClient(fields);
		this.buildConnection(fields ?? {}, client);
	}
	getLsParams(options) {
		const params = this.invocationParams(options);
		return {
			ls_provider: "google_vertexai",
			ls_model_name: this.model,
			ls_model_type: "chat",
			ls_temperature: params.temperature ?? void 0,
			ls_max_tokens: params.maxOutputTokens ?? void 0,
			ls_stop: options.stop
		};
	}
	buildApiKeyClient(apiKey) {
		return new require_auth.ApiKeyGoogleAuth(apiKey);
	}
	buildApiKey(fields) {
		return fields?.apiKey ?? (0, __langchain_core_utils_env.getEnvironmentVariable)("GOOGLE_API_KEY");
	}
	buildClient(fields) {
		const apiKey = this.buildApiKey(fields);
		if (apiKey) return this.buildApiKeyClient(apiKey);
		else return this.buildAbstractedClient(fields);
	}
	buildConnection(fields, client) {
		this.connection = new ChatConnection({
			...fields,
			...this
		}, this.caller, client, false);
		this.streamedConnection = new ChatConnection({
			...fields,
			...this
		}, this.caller, client, true);
	}
	get platform() {
		return this.connection.platform;
	}
	bindTools(tools, kwargs) {
		return this.withConfig({
			tools: require_common.convertToGeminiTools(tools),
			...kwargs
		});
	}
	_llmType() {
		return "chat_integration";
	}
	/**
	* Get the parameters used to invoke the model
	*/
	invocationParams(options) {
		return require_common.copyAIModelParams(this, options);
	}
	async _generate(messages, options, runManager) {
		const parameters = this.invocationParams(options);
		if (this.streaming) {
			const stream = this._streamResponseChunks(messages, options, runManager);
			let finalChunk = null;
			for await (const chunk$1 of stream) finalChunk = !finalChunk ? chunk$1 : (0, __langchain_core_utils_stream.concat)(finalChunk, chunk$1);
			if (!finalChunk) throw new Error("No chunks were returned from the stream.");
			return { generations: [finalChunk] };
		}
		const response = await this.connection.request(messages, parameters, options, runManager);
		const ret = this.connection.api.responseToChatResult(response);
		const chunk = ret?.generations?.[0];
		if (chunk) await runManager?.handleLLMNewToken(chunk.text || "");
		return ret;
	}
	async *_streamResponseChunks(_messages, options, runManager) {
		const parameters = this.invocationParams(options);
		const response = await this.streamedConnection.request(_messages, parameters, options, runManager);
		const stream = response.data;
		let usageMetadata;
		while (!stream.streamDone) {
			const output = await stream.nextChunk();
			await runManager?.handleCustomEvent(`google-chunk-${this.constructor.name}`, { output });
			if (output && output.usageMetadata && this.streamUsage !== false && options.streamUsage !== false) usageMetadata = {
				input_tokens: output.usageMetadata.promptTokenCount,
				output_tokens: output.usageMetadata.candidatesTokenCount,
				total_tokens: output.usageMetadata.totalTokenCount
			};
			const chunk = output !== null ? this.connection.api.responseToChatGeneration({ data: output }) : new __langchain_core_outputs.ChatGenerationChunk({
				text: "",
				generationInfo: { finishReason: "stop" },
				message: new __langchain_core_messages.AIMessageChunk({
					content: "",
					usage_metadata: usageMetadata
				})
			});
			if (chunk) {
				yield chunk;
				await runManager?.handleLLMNewToken(chunk.text ?? "", void 0, void 0, void 0, void 0, { chunk });
			}
		}
	}
	/** @ignore */
	_combineLLMOutput() {
		return [];
	}
	/**
	* Return profiling information for the model.
	*
	* Provides information about the model's capabilities and constraints,
	* including token limits, multimodal support, and advanced features like
	* tool calling and structured output.
	*
	* @returns {ModelProfile} An object describing the model's capabilities and constraints
	*/
	get profile() {
		return require_profiles.default[this.model] ?? {};
	}
	withStructuredOutput(outputSchema, config) {
		const schema = outputSchema;
		const name = config?.name;
		const method = config?.method;
		const includeRaw = config?.includeRaw;
		if (method === "jsonMode") throw new Error(`Google only supports "functionCalling" as a method.`);
		let functionName = name ?? "extract";
		let outputParser;
		let tools;
		if ((0, __langchain_core_utils_types.isInteropZodSchema)(schema)) {
			const jsonSchema = require_zod_to_gemini_parameters.schemaToGeminiParameters(schema);
			tools = [{ functionDeclarations: [{
				name: functionName,
				description: jsonSchema.description ?? "A function available to call.",
				parameters: jsonSchema
			}] }];
			outputParser = new __langchain_core_output_parsers_openai_tools.JsonOutputKeyToolsParser({
				returnSingle: true,
				keyName: functionName,
				zodSchema: schema
			});
		} else {
			let geminiFunctionDefinition;
			if (typeof schema.name === "string" && typeof schema.parameters === "object" && schema.parameters != null) {
				geminiFunctionDefinition = schema;
				functionName = schema.name;
			} else {
				const parameters = require_zod_to_gemini_parameters.removeAdditionalProperties(schema);
				geminiFunctionDefinition = {
					name: functionName,
					description: schema.description ?? "",
					parameters
				};
			}
			tools = [{ functionDeclarations: [geminiFunctionDefinition] }];
			outputParser = new __langchain_core_output_parsers_openai_tools.JsonOutputKeyToolsParser({
				returnSingle: true,
				keyName: functionName
			});
		}
		const llm = this.bindTools(tools).withConfig({ tool_choice: functionName });
		if (!includeRaw) return llm.pipe(outputParser).withConfig({ runName: "ChatGoogleStructuredOutput" });
		const parserAssign = __langchain_core_runnables.RunnablePassthrough.assign({ parsed: (input, config$1) => outputParser.invoke(input.raw, config$1) });
		const parserNone = __langchain_core_runnables.RunnablePassthrough.assign({ parsed: () => null });
		const parsedWithFallback = parserAssign.withFallbacks({ fallbacks: [parserNone] });
		return __langchain_core_runnables.RunnableSequence.from([{ raw: llm }, parsedWithFallback]).withConfig({ runName: "StructuredOutputRunnable" });
	}
};

//#endregion
exports.ChatConnection = ChatConnection;
exports.ChatGoogleBase = ChatGoogleBase;
//# sourceMappingURL=chat_models.cjs.map