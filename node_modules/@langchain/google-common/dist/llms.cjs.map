{"version":3,"file":"llms.cjs","names":["AbstractGoogleLLMConnection","input: MessageContent","_parameters: GoogleAIModelParams","contents: GeminiContent[]","ChatGoogleBase","fields: ProxyChatInput<AuthOptions>","LLM","fields?: GoogleBaseLLMInput<AuthOptions>","ensureParams","copyAndValidateModelParamsInto","DefaultGeminiSafetyHandler","apiKey: string","ApiKeyGoogleAuth","fields?: GoogleAIBaseLLMInput<AuthOptions>","fields: GoogleBaseLLMInput<AuthOptions>","client: GoogleAbstractedClient","prompt: string","options: this[\"ParsedCallOptions\"]","copyAIModelParams","input: BaseLanguageModelInput","options?: BaseLanguageModelCallOptions","BaseLLM","CallbackManager","GenerationChunk","messages: BaseMessage[]","options?: string[] | BaseLanguageModelCallOptions","_callbacks?: Callbacks"],"sources":["../src/llms.ts"],"sourcesContent":["import { CallbackManager, Callbacks } from \"@langchain/core/callbacks/manager\";\nimport { BaseLLM, LLM } from \"@langchain/core/language_models/llms\";\nimport {\n  type BaseLanguageModelCallOptions,\n  BaseLanguageModelInput,\n} from \"@langchain/core/language_models/base\";\nimport { BaseMessage, MessageContent } from \"@langchain/core/messages\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n\nimport { AbstractGoogleLLMConnection } from \"./connection.js\";\nimport {\n  GoogleAIBaseLLMInput,\n  GoogleAIModelParams,\n  GoogleAISafetySetting,\n  GooglePlatformType,\n  GeminiContent,\n  GoogleAIResponseMimeType,\n} from \"./types.js\";\nimport {\n  copyAIModelParams,\n  copyAndValidateModelParamsInto,\n} from \"./utils/common.js\";\nimport { DefaultGeminiSafetyHandler } from \"./utils/gemini.js\";\nimport { ApiKeyGoogleAuth, GoogleAbstractedClient } from \"./auth.js\";\nimport { ensureParams } from \"./utils/failed_handler.js\";\nimport { ChatGoogleBase } from \"./chat_models.js\";\nimport type { GoogleBaseLLMInput, GoogleAISafetyHandler } from \"./types.js\";\n\nexport { GoogleBaseLLMInput };\n\nclass GoogleLLMConnection<AuthOptions> extends AbstractGoogleLLMConnection<\n  MessageContent,\n  AuthOptions\n> {\n  async formatContents(\n    input: MessageContent,\n    _parameters: GoogleAIModelParams\n  ): Promise<GeminiContent[]> {\n    const parts = await this.api.messageContentToParts!(input);\n    const contents: GeminiContent[] = [\n      {\n        role: \"user\", // Required by Vertex AI\n        parts,\n      },\n    ];\n    return contents;\n  }\n}\n\ntype ProxyChatInput<AuthOptions> = GoogleAIBaseLLMInput<AuthOptions> & {\n  connection: GoogleLLMConnection<AuthOptions>;\n};\n\nclass ProxyChatGoogle<AuthOptions> extends ChatGoogleBase<AuthOptions> {\n  constructor(fields: ProxyChatInput<AuthOptions>) {\n    super(fields);\n  }\n\n  buildAbstractedClient(\n    fields: ProxyChatInput<AuthOptions>\n  ): GoogleAbstractedClient {\n    return fields.connection.client;\n  }\n}\n\n/**\n * Integration with an LLM.\n */\nexport abstract class GoogleBaseLLM<AuthOptions>\n  extends LLM<BaseLanguageModelCallOptions>\n  implements GoogleBaseLLMInput<AuthOptions>\n{\n  // Used for tracing, replace with the same name as your class\n  static lc_name() {\n    return \"GoogleLLM\";\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      authOptions: \"GOOGLE_AUTH_OPTIONS\",\n    };\n  }\n\n  originalFields?: GoogleBaseLLMInput<AuthOptions>;\n\n  lc_serializable = true;\n\n  modelName = \"gemini-pro\";\n\n  model = \"gemini-pro\";\n\n  temperature = 0.7;\n\n  maxOutputTokens = 1024;\n\n  topP = 0.8;\n\n  topK = 40;\n\n  stopSequences: string[] = [];\n\n  safetySettings: GoogleAISafetySetting[] = [];\n\n  safetyHandler: GoogleAISafetyHandler;\n\n  responseMimeType: GoogleAIResponseMimeType = \"text/plain\";\n\n  protected connection: GoogleLLMConnection<AuthOptions>;\n\n  protected streamedConnection: GoogleLLMConnection<AuthOptions>;\n\n  constructor(fields?: GoogleBaseLLMInput<AuthOptions>) {\n    super(ensureParams(fields));\n    this.originalFields = fields;\n\n    copyAndValidateModelParamsInto(fields, this);\n    this.safetyHandler =\n      fields?.safetyHandler ?? new DefaultGeminiSafetyHandler();\n\n    const client = this.buildClient(fields);\n    this.buildConnection(fields ?? {}, client);\n  }\n\n  abstract buildAbstractedClient(\n    fields?: GoogleAIBaseLLMInput<AuthOptions>\n  ): GoogleAbstractedClient;\n\n  buildApiKeyClient(apiKey: string): GoogleAbstractedClient {\n    return new ApiKeyGoogleAuth(apiKey);\n  }\n\n  buildApiKey(fields?: GoogleAIBaseLLMInput<AuthOptions>): string | undefined {\n    return fields?.apiKey ?? getEnvironmentVariable(\"GOOGLE_API_KEY\");\n  }\n\n  buildClient(\n    fields?: GoogleAIBaseLLMInput<AuthOptions>\n  ): GoogleAbstractedClient {\n    const apiKey = this.buildApiKey(fields);\n    if (apiKey) {\n      return this.buildApiKeyClient(apiKey);\n    } else {\n      return this.buildAbstractedClient(fields);\n    }\n  }\n\n  buildConnection(\n    fields: GoogleBaseLLMInput<AuthOptions>,\n    client: GoogleAbstractedClient\n  ) {\n    this.connection = new GoogleLLMConnection(\n      { ...fields, ...this },\n      this.caller,\n      client,\n      false\n    );\n\n    this.streamedConnection = new GoogleLLMConnection(\n      { ...fields, ...this },\n      this.caller,\n      client,\n      true\n    );\n  }\n\n  get platform(): GooglePlatformType {\n    return this.connection.platform;\n  }\n\n  // Replace\n  _llmType() {\n    return \"googlellm\";\n  }\n\n  formatPrompt(prompt: string): MessageContent {\n    return prompt;\n  }\n\n  /**\n   * For some given input string and options, return a string output.\n   *\n   * Despite the fact that `invoke` is overridden below, we still need this\n   * in order to handle public APi calls to `generate()`.\n   */\n  async _call(\n    prompt: string,\n    options: this[\"ParsedCallOptions\"]\n  ): Promise<string> {\n    const parameters = copyAIModelParams(this, options);\n    const result = await this.connection.request(prompt, parameters, options);\n    const ret = this.connection.api.responseToString(result);\n    return ret;\n  }\n\n  // Normally, you should not override this method and instead should override\n  // _streamResponseChunks. We are doing so here to allow for multimodal inputs into\n  // the LLM.\n  async *_streamIterator(\n    input: BaseLanguageModelInput,\n    options?: BaseLanguageModelCallOptions\n  ): AsyncGenerator<string> {\n    // TODO: Refactor callback setup and teardown code into core\n    const prompt = BaseLLM._convertInputToPromptValue(input);\n    const [runnableConfig, callOptions] =\n      this._separateRunnableConfigFromCallOptions(options);\n    const callbackManager_ = await CallbackManager.configure(\n      runnableConfig.callbacks,\n      this.callbacks,\n      runnableConfig.tags,\n      this.tags,\n      runnableConfig.metadata,\n      this.metadata,\n      { verbose: this.verbose }\n    );\n    const extra = {\n      options: callOptions,\n      invocation_params: this?.invocationParams(callOptions),\n      batch_size: 1,\n    };\n    const runManagers = await callbackManager_?.handleLLMStart(\n      this.toJSON(),\n      [prompt.toString()],\n      undefined,\n      undefined,\n      extra,\n      undefined,\n      undefined,\n      runnableConfig.runName\n    );\n    let generation = new GenerationChunk({\n      text: \"\",\n    });\n    const proxyChat = this.createProxyChat();\n    try {\n      for await (const chunk of proxyChat._streamIterator(input, options)) {\n        const stringValue = this.connection.api.chunkToString(chunk);\n        const generationChunk = new GenerationChunk({\n          text: stringValue,\n        });\n        generation = generation.concat(generationChunk);\n        yield stringValue;\n      }\n    } catch (err) {\n      await Promise.all(\n        (runManagers ?? []).map((runManager) => runManager?.handleLLMError(err))\n      );\n      throw err;\n    }\n    await Promise.all(\n      (runManagers ?? []).map((runManager) =>\n        runManager?.handleLLMEnd({\n          generations: [[generation]],\n        })\n      )\n    );\n  }\n\n  async predictMessages(\n    messages: BaseMessage[],\n    options?: string[] | BaseLanguageModelCallOptions,\n    _callbacks?: Callbacks\n  ): Promise<BaseMessage> {\n    const { content } = messages[0];\n    const result = await this.connection.request(\n      content,\n      {},\n      options as BaseLanguageModelCallOptions\n    );\n    const ret = this.connection.api.responseToBaseMessage(result);\n    return ret;\n  }\n\n  /**\n   * Internal implementation detail to allow Google LLMs to support\n   * multimodal input by delegating to the chat model implementation.\n   *\n   * TODO: Replace with something less hacky.\n   */\n  protected createProxyChat(): ChatGoogleBase<AuthOptions> {\n    return new ProxyChatGoogle<AuthOptions>({\n      ...this.originalFields,\n      connection: this.connection,\n    });\n  }\n\n  // TODO: Remove the need to override this - we are doing it to\n  // allow the LLM to handle multimodal types of input.\n  async invoke(\n    input: BaseLanguageModelInput,\n    options?: BaseLanguageModelCallOptions\n  ): Promise<string> {\n    const stream = await this._streamIterator(input, options);\n    let generatedOutput = \"\";\n    for await (const chunk of stream) {\n      generatedOutput += chunk;\n    }\n    return generatedOutput;\n  }\n}\n"],"mappings":";;;;;;;;;;;;;AA+BA,IAAM,sBAAN,cAA+CA,+CAG7C;CACA,MAAM,eACJC,OACAC,aAC0B;EAC1B,MAAM,QAAQ,MAAM,KAAK,IAAI,sBAAuB,MAAM;EAC1D,MAAMC,WAA4B,CAChC;GACE,MAAM;GACN;EACD,CACF;AACD,SAAO;CACR;AACF;AAMD,IAAM,kBAAN,cAA2CC,mCAA4B;CACrE,YAAYC,QAAqC;EAC/C,MAAM,OAAO;CACd;CAED,sBACEA,QACwB;AACxB,SAAO,OAAO,WAAW;CAC1B;AACF;;;;AAKD,IAAsB,gBAAtB,cACUC,0CAEV;CAEE,OAAO,UAAU;AACf,SAAO;CACR;CAED,IAAI,aAAoD;AACtD,SAAO,EACL,aAAa,sBACd;CACF;CAED;CAEA,kBAAkB;CAElB,YAAY;CAEZ,QAAQ;CAER,cAAc;CAEd,kBAAkB;CAElB,OAAO;CAEP,OAAO;CAEP,gBAA0B,CAAE;CAE5B,iBAA0C,CAAE;CAE5C;CAEA,mBAA6C;CAE7C,AAAU;CAEV,AAAU;CAEV,YAAYC,QAA0C;EACpD,MAAMC,oCAAa,OAAO,CAAC;EAC3B,KAAK,iBAAiB;EAEtBC,8CAA+B,QAAQ,KAAK;EAC5C,KAAK,gBACH,QAAQ,iBAAiB,IAAIC;EAE/B,MAAM,SAAS,KAAK,YAAY,OAAO;EACvC,KAAK,gBAAgB,UAAU,CAAE,GAAE,OAAO;CAC3C;CAMD,kBAAkBC,QAAwC;AACxD,SAAO,IAAIC,8BAAiB;CAC7B;CAED,YAAYC,QAAgE;AAC1E,SAAO,QAAQ,iEAAiC,iBAAiB;CAClE;CAED,YACEA,QACwB;EACxB,MAAM,SAAS,KAAK,YAAY,OAAO;AACvC,MAAI,OACF,QAAO,KAAK,kBAAkB,OAAO;MAErC,QAAO,KAAK,sBAAsB,OAAO;CAE5C;CAED,gBACEC,QACAC,QACA;EACA,KAAK,aAAa,IAAI,oBACpB;GAAE,GAAG;GAAQ,GAAG;EAAM,GACtB,KAAK,QACL,QACA;EAGF,KAAK,qBAAqB,IAAI,oBAC5B;GAAE,GAAG;GAAQ,GAAG;EAAM,GACtB,KAAK,QACL,QACA;CAEH;CAED,IAAI,WAA+B;AACjC,SAAO,KAAK,WAAW;CACxB;CAGD,WAAW;AACT,SAAO;CACR;CAED,aAAaC,QAAgC;AAC3C,SAAO;CACR;;;;;;;CAQD,MAAM,MACJA,QACAC,SACiB;EACjB,MAAM,aAAaC,iCAAkB,MAAM,QAAQ;EACnD,MAAM,SAAS,MAAM,KAAK,WAAW,QAAQ,QAAQ,YAAY,QAAQ;EACzE,MAAM,MAAM,KAAK,WAAW,IAAI,iBAAiB,OAAO;AACxD,SAAO;CACR;CAKD,OAAO,gBACLC,OACAC,SACwB;EAExB,MAAM,SAASC,8CAAQ,2BAA2B,MAAM;EACxD,MAAM,CAAC,gBAAgB,YAAY,GACjC,KAAK,uCAAuC,QAAQ;EACtD,MAAM,mBAAmB,MAAMC,mDAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,eAAe,UACf,KAAK,UACL,EAAE,SAAS,KAAK,QAAS,EAC1B;EACD,MAAM,QAAQ;GACZ,SAAS;GACT,mBAAmB,MAAM,iBAAiB,YAAY;GACtD,YAAY;EACb;EACD,MAAM,cAAc,MAAM,kBAAkB,eAC1C,KAAK,QAAQ,EACb,CAAC,OAAO,UAAU,AAAC,GACnB,QACA,QACA,OACA,QACA,QACA,eAAe,QAChB;EACD,IAAI,aAAa,IAAIC,yCAAgB,EACnC,MAAM,GACP;EACD,MAAM,YAAY,KAAK,iBAAiB;AACxC,MAAI;AACF,cAAW,MAAM,SAAS,UAAU,gBAAgB,OAAO,QAAQ,EAAE;IACnE,MAAM,cAAc,KAAK,WAAW,IAAI,cAAc,MAAM;IAC5D,MAAM,kBAAkB,IAAIA,yCAAgB,EAC1C,MAAM,YACP;IACD,aAAa,WAAW,OAAO,gBAAgB;IAC/C,MAAM;GACP;EACF,SAAQ,KAAK;GACZ,MAAM,QAAQ,KACX,eAAe,CAAE,GAAE,IAAI,CAAC,eAAe,YAAY,eAAe,IAAI,CAAC,CACzE;AACD,SAAM;EACP;EACD,MAAM,QAAQ,KACX,eAAe,CAAE,GAAE,IAAI,CAAC,eACvB,YAAY,aAAa,EACvB,aAAa,CAAC,CAAC,UAAW,CAAC,EAC5B,EAAC,CACH,CACF;CACF;CAED,MAAM,gBACJC,UACAC,SACAC,YACsB;EACtB,MAAM,EAAE,SAAS,GAAG,SAAS;EAC7B,MAAM,SAAS,MAAM,KAAK,WAAW,QACnC,SACA,CAAE,GACF,QACD;EACD,MAAM,MAAM,KAAK,WAAW,IAAI,sBAAsB,OAAO;AAC7D,SAAO;CACR;;;;;;;CAQD,AAAU,kBAA+C;AACvD,SAAO,IAAI,gBAA6B;GACtC,GAAG,KAAK;GACR,YAAY,KAAK;EAClB;CACF;CAID,MAAM,OACJP,OACAC,SACiB;EACjB,MAAM,SAAS,MAAM,KAAK,gBAAgB,OAAO,QAAQ;EACzD,IAAI,kBAAkB;AACtB,aAAW,MAAM,SAAS,QACxB,mBAAmB;AAErB,SAAO;CACR;AACF"}