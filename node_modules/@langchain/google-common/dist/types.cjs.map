{"version":3,"file":"types.cjs","names":[],"sources":["../src/types.ts"],"sourcesContent":["import type { BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport type {\n  BaseChatModelCallOptions,\n  BindToolsInput,\n} from \"@langchain/core/language_models/chat_models\";\nimport {\n  BaseMessage,\n  BaseMessageChunk,\n  MessageContent,\n} from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, ChatResult } from \"@langchain/core/outputs\";\nimport { EmbeddingsParams } from \"@langchain/core/embeddings\";\nimport { AsyncCallerCallOptions } from \"@langchain/core/utils/async_caller\";\nimport type { JsonStream } from \"./utils/stream.js\";\nimport { MediaManager } from \"./experimental/utils/media_core.js\";\nimport {\n  AnthropicResponseData,\n  AnthropicAPIConfig,\n} from \"./types-anthropic.js\";\n\nexport * from \"./types-anthropic.js\";\n\n/**\n * Parameters needed to setup the client connection.\n * AuthOptions are something like GoogleAuthOptions (from google-auth-library)\n * or WebGoogleAuthOptions.\n */\nexport interface GoogleClientParams<AuthOptions> {\n  authOptions?: AuthOptions;\n\n  /** Some APIs allow an API key instead */\n  apiKey?: string;\n}\n\n/**\n * What platform is this running on?\n * gai - Google AI Studio / MakerSuite / Generative AI platform\n * gcp - Google Cloud Platform\n */\nexport type GooglePlatformType = \"gai\" | \"gcp\";\n\nexport interface GoogleConnectionParams<AuthOptions>\n  extends GoogleClientParams<AuthOptions> {\n  /** Hostname for the API call (if this is running on GCP) */\n  endpoint?: string;\n\n  /** Region where the LLM is stored (if this is running on GCP) */\n  location?: string;\n\n  /** The version of the API functions. Part of the path. */\n  apiVersion?: string;\n\n  /**\n   * What platform to run the service on.\n   * If not specified, the class should determine this from other\n   * means. Either way, the platform actually used will be in\n   * the \"platform\" getter.\n   */\n  platformType?: GooglePlatformType;\n\n  /**\n   * For compatibility with Google's libraries, should this use Vertex?\n   * The \"platformType\" parmeter takes precedence.\n   */\n  vertexai?: boolean;\n}\n\nexport const GoogleAISafetyCategory = {\n  Harassment: \"HARM_CATEGORY_HARASSMENT\",\n  HARASSMENT: \"HARM_CATEGORY_HARASSMENT\",\n  HARM_CATEGORY_HARASSMENT: \"HARM_CATEGORY_HARASSMENT\",\n\n  HateSpeech: \"HARM_CATEGORY_HATE_SPEECH\",\n  HATE_SPEECH: \"HARM_CATEGORY_HATE_SPEECH\",\n  HARM_CATEGORY_HATE_SPEECH: \"HARM_CATEGORY_HATE_SPEECH\",\n\n  SexuallyExplicit: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n  SEXUALLY_EXPLICIT: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n  HARM_CATEGORY_SEXUALLY_EXPLICIT: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n\n  Dangerous: \"HARM_CATEGORY_DANGEROUS\",\n  DANGEROUS: \"HARM_CATEGORY_DANGEROUS\",\n  HARM_CATEGORY_DANGEROUS: \"HARM_CATEGORY_DANGEROUS\",\n\n  CivicIntegrity: \"HARM_CATEGORY_CIVIC_INTEGRITY\",\n  CIVIC_INTEGRITY: \"HARM_CATEGORY_CIVIC_INTEGRITY\",\n  HARM_CATEGORY_CIVIC_INTEGRITY: \"HARM_CATEGORY_CIVIC_INTEGRITY\",\n} as const;\n\nexport type GoogleAISafetyCategory =\n  (typeof GoogleAISafetyCategory)[keyof typeof GoogleAISafetyCategory];\n\nexport const GoogleAISafetyThreshold = {\n  None: \"BLOCK_NONE\",\n  NONE: \"BLOCK_NONE\",\n  BLOCK_NONE: \"BLOCK_NONE\",\n\n  Few: \"BLOCK_ONLY_HIGH\",\n  FEW: \"BLOCK_ONLY_HIGH\",\n  BLOCK_ONLY_HIGH: \"BLOCK_ONLY_HIGH\",\n\n  Some: \"BLOCK_MEDIUM_AND_ABOVE\",\n  SOME: \"BLOCK_MEDIUM_AND_ABOVE\",\n  BLOCK_MEDIUM_AND_ABOVE: \"BLOCK_MEDIUM_AND_ABOVE\",\n\n  Most: \"BLOCK_LOW_AND_ABOVE\",\n  MOST: \"BLOCK_LOW_AND_ABOVE\",\n  BLOCK_LOW_AND_ABOVE: \"BLOCK_LOW_AND_ABOVE\",\n\n  Off: \"OFF\",\n  OFF: \"OFF\",\n  BLOCK_OFF: \"OFF\",\n} as const;\n\nexport type GoogleAISafetyThreshold =\n  (typeof GoogleAISafetyThreshold)[keyof typeof GoogleAISafetyThreshold];\n\nexport const GoogleAISafetyMethod = {\n  Severity: \"SEVERITY\",\n  Probability: \"PROBABILITY\",\n} as const;\n\nexport type GoogleAISafetyMethod =\n  (typeof GoogleAISafetyMethod)[keyof typeof GoogleAISafetyMethod];\n\nexport interface GoogleAISafetySetting {\n  category: GoogleAISafetyCategory | string;\n  threshold: GoogleAISafetyThreshold | string;\n  method?: GoogleAISafetyMethod | string; // Just for Vertex AI?\n}\n\nexport type GoogleAIResponseMimeType = \"text/plain\" | \"application/json\";\n\nexport type GoogleAIModelModality = \"TEXT\" | \"IMAGE\" | \"AUDIO\" | string;\n\nexport interface GoogleThinkingConfig {\n  thinkingBudget?: number;\n  includeThoughts?: boolean;\n}\n\nexport type GooglePrebuiltVoiceName = string;\n\nexport interface GooglePrebuiltVoiceConfig {\n  voiceName: GooglePrebuiltVoiceName;\n}\n\nexport interface GoogleVoiceConfig {\n  prebuiltVoiceConfig: GooglePrebuiltVoiceConfig;\n}\n\nexport interface GoogleSpeakerVoiceConfig {\n  speaker: string;\n  voiceConfig: GoogleVoiceConfig;\n}\n\nexport interface GoogleMultiSpeakerVoiceConfig {\n  speakerVoiceConfigs: GoogleSpeakerVoiceConfig[];\n}\n\nexport interface GoogleSpeechConfigSingle {\n  voiceConfig: GoogleVoiceConfig;\n  languageCode?: string;\n}\n\nexport interface GoogleSpeechConfigMulti {\n  multiSpeakerVoiceConfig: GoogleMultiSpeakerVoiceConfig;\n  languageCode?: string;\n}\n\nexport type GoogleSpeechConfig =\n  | GoogleSpeechConfigSingle\n  | GoogleSpeechConfigMulti;\n\n/**\n * A simplified version of the GoogleSpeakerVoiceConfig\n */\nexport interface GoogleSpeechSpeakerName {\n  speaker: string;\n  name: GooglePrebuiltVoiceName;\n}\n\nexport type GoogleSpeechVoice =\n  | GooglePrebuiltVoiceName\n  | GoogleSpeechSpeakerName\n  | GoogleSpeechSpeakerName[];\n\nexport interface GoogleSpeechVoiceLanguage {\n  voice: GoogleSpeechVoice;\n  languageCode: string;\n}\n\nexport interface GoogleSpeechVoicesLanguage {\n  voices: GoogleSpeechVoice;\n  languageCode: string;\n}\n\n/**\n * A simplified way to represent the voice (or voices) and language code.\n * \"voice\" and \"voices\" are semantically the same, we're not enforcing\n * that one is an array and one isn't.\n */\nexport type GoogleSpeechSimplifiedLanguage =\n  | GoogleSpeechVoiceLanguage\n  | GoogleSpeechVoicesLanguage;\n\n/**\n * A simplified way to represent the voices.\n * It can either be the voice (or voices), or the voice or voices with language configuration\n */\nexport type GoogleSpeechConfigSimplified =\n  | GoogleSpeechVoice\n  | GoogleSpeechSimplifiedLanguage;\n\nexport interface GoogleModelParams {\n  /** Model to use */\n  model?: string;\n\n  /**\n   * Model to use\n   * Alias for `model`\n   */\n  modelName?: string;\n}\n\nexport interface GoogleAIModelParams extends GoogleModelParams {\n  /** Sampling temperature to use */\n  temperature?: number;\n\n  /**\n   * Maximum number of tokens to generate in the completion.\n   * This may include reasoning tokens (for backwards compatibility).\n   */\n  maxOutputTokens?: number;\n\n  /**\n   * The maximum number of the output tokens that will be used\n   * for the \"thinking\" or \"reasoning\" stages.\n   */\n  maxReasoningTokens?: number;\n\n  /**\n   * An alias for \"maxReasoningTokens\"\n   */\n  thinkingBudget?: number;\n\n  /**\n   * An OpenAI compatible parameter that will map to \"maxReasoningTokens\"\n   */\n  reasoningEffort?: \"low\" | \"medium\" | \"high\";\n\n  /**\n   * Top-p changes how the model selects tokens for output.\n   *\n   * Tokens are selected from most probable to least until the sum\n   * of their probabilities equals the top-p value.\n   *\n   * For example, if tokens A, B, and C have a probability of\n   * .3, .2, and .1 and the top-p value is .5, then the model will\n   * select either A or B as the next token (using temperature).\n   */\n  topP?: number;\n\n  /**\n   * Top-k changes how the model selects tokens for output.\n   *\n   * A top-k of 1 means the selected token is the most probable among\n   * all tokens in the modelâ€™s vocabulary (also called greedy decoding),\n   * while a top-k of 3 means that the next token is selected from\n   * among the 3 most probable tokens (using temperature).\n   */\n  topK?: number;\n\n  /**\n   * Seed used in decoding. If not set, the request uses a randomly generated seed.\n   */\n  seed?: number;\n\n  /**\n   * Presence penalty applied to the next token's logprobs\n   * if the token has already been seen in the response.\n   * This penalty is binary on/off and not dependant on the\n   * number of times the token is used (after the first).\n   * Use frequencyPenalty for a penalty that increases with each use.\n   * A positive penalty will discourage the use of tokens that have\n   * already been used in the response, increasing the vocabulary.\n   * A negative penalty will encourage the use of tokens that have\n   * already been used in the response, decreasing the vocabulary.\n   */\n  presencePenalty?: number;\n\n  /**\n   * Frequency penalty applied to the next token's logprobs,\n   * multiplied by the number of times each token has been seen\n   * in the respponse so far.\n   * A positive penalty will discourage the use of tokens that\n   * have already been used, proportional to the number of times\n   * the token has been used:\n   * The more a token is used, the more dificult it is for the model\n   * to use that token again increasing the vocabulary of responses.\n   * Caution: A _negative_ penalty will encourage the model to reuse\n   * tokens proportional to the number of times the token has been used.\n   * Small negative values will reduce the vocabulary of a response.\n   * Larger negative values will cause the model to start repeating\n   * a common token until it hits the maxOutputTokens limit.\n   */\n  frequencyPenalty?: number;\n\n  stopSequences?: string[];\n\n  safetySettings?: GoogleAISafetySetting[];\n\n  convertSystemMessageToHumanContent?: boolean;\n\n  /**\n   * Available for `gemini-1.5-pro`.\n   * The output format of the generated candidate text.\n   * Supported MIME types:\n   *  - `text/plain`: Text output.\n   *  - `application/json`: JSON response in the candidates.\n   *\n   * @default \"text/plain\"\n   */\n  responseMimeType?: GoogleAIResponseMimeType;\n\n  /**\n   * Whether or not to stream.\n   * @default false\n   */\n  streaming?: boolean;\n\n  /**\n   * Whether to return log probabilities of the output tokens or not.\n   * If true, returns the log probabilities of each output token\n   * returned in the content of message.\n   */\n  logprobs?: boolean;\n\n  /**\n   * An integer between 0 and 5 specifying the number of\n   * most likely tokens to return at each token position,\n   * each with an associated log probability.\n   * logprobs must be set to true if this parameter is used.\n   */\n  topLogprobs?: number;\n\n  /**\n   * The modalities of the response.\n   */\n  responseModalities?: GoogleAIModelModality[];\n\n  /**\n   * Custom metadata labels to associate with the request.\n   * Only supported on Vertex AI (Google Cloud Platform).\n   * Labels are key-value pairs where both keys and values must be strings.\n   *\n   * Example:\n   * ```typescript\n   * {\n   *   labels: {\n   *     \"team\": \"research\",\n   *     \"component\": \"frontend\",\n   *     \"environment\": \"production\"\n   *   }\n   * }\n   * ```\n   */\n  labels?: Record<string, string>;\n\n  /**\n   * Speech generation configuration.\n   * You can use either Google's definition of the speech configuration,\n   * or a simplified version we've defined (which can be as simple\n   * as the name of a pre-defined voice).\n   */\n  speechConfig?: GoogleSpeechConfig | GoogleSpeechConfigSimplified;\n}\n\nexport type GoogleAIToolType = BindToolsInput | GeminiTool;\n\n/**\n * The params which can be passed to the API at request time.\n */\nexport interface GoogleAIModelRequestParams extends GoogleAIModelParams {\n  tools?: GoogleAIToolType[];\n  /**\n   * Force the model to use tools in a specific way.\n   *\n   * | Mode     |\tDescription                                                                                                                                             |\n   * |----------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n   * | \"auto\"\t  | The default model behavior. The model decides whether to predict a function call or a natural language response.                                        |\n   * | \"any\"\t  | The model must predict only function calls. To limit the model to a subset of functions, define the allowed function names in `allowed_function_names`. |\n   * | \"none\"\t  | The model must not predict function calls. This behavior is equivalent to a model request without any associated function declarations.                 |\n   * | string   | The string value must be one of the function names. This will force the model to predict the specified function call.                                   |\n   *\n   * The tool configuration's \"any\" mode (\"forced function calling\") is supported for Gemini 1.5 Pro models only.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  tool_choice?: string | \"auto\" | \"any\" | \"none\" | Record<string, any>;\n  /**\n   * Allowed functions to call when the mode is \"any\".\n   * If empty, any one of the provided functions are called.\n   */\n  allowed_function_names?: string[];\n\n  /**\n   * Used to specify a previously created context cache to use with generation.\n   * For Vertex, this should be of the form:\n   * \"projects/PROJECT_NUMBER/locations/LOCATION/cachedContents/CACHE_ID\",\n   *\n   * See these guides for more information on how to use context caching:\n   * https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-create\n   * https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-use\n   */\n  cachedContent?: string;\n}\n\nexport interface GoogleAIBaseLLMInput<AuthOptions>\n  extends BaseLLMParams,\n    GoogleConnectionParams<AuthOptions>,\n    GoogleAIModelParams,\n    GoogleAISafetyParams,\n    GoogleAIAPIParams {}\n\nexport interface GoogleAIBaseLanguageModelCallOptions\n  extends BaseChatModelCallOptions,\n    GoogleAIModelRequestParams,\n    GoogleAISafetyParams {\n  /**\n   * Whether or not to include usage data, like token counts\n   * in the streamed response chunks.\n   * @default true\n   */\n  streamUsage?: boolean;\n}\n\n/**\n * Input to LLM class.\n */\nexport interface GoogleBaseLLMInput<AuthOptions>\n  extends GoogleAIBaseLLMInput<AuthOptions> {}\n\nexport interface GoogleResponse {\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  data: any;\n}\n\nexport interface GoogleRawResponse extends GoogleResponse {\n  data: Blob;\n}\n\nexport interface GeminiPartBase {\n  thought?: boolean; // Output only\n  thoughtSignature?: string;\n}\n\nexport interface GeminiVideoMetadata {\n  fps?: number; // Double in range (0.0, 24.0]\n  startOffset?: string;\n  endOffset?: string;\n}\n\nexport interface GeminiPartBaseFile extends GeminiPartBase {\n  videoMetadata?: GeminiVideoMetadata;\n}\n\nexport interface GeminiPartText extends GeminiPartBase {\n  text: string;\n}\n\nexport interface GeminiPartInlineData extends GeminiPartBaseFile {\n  inlineData: {\n    mimeType: string;\n    data: string;\n  };\n}\n\nexport interface GeminiPartFileData extends GeminiPartBaseFile {\n  fileData: {\n    mimeType: string;\n    fileUri: string;\n  };\n}\n\n// AI Studio only?\nexport interface GeminiPartFunctionCall extends GeminiPartBase {\n  functionCall: {\n    name: string;\n    args?: object;\n  };\n}\n\n// AI Studio Only?\nexport interface GeminiPartFunctionResponse extends GeminiPartBase {\n  functionResponse: {\n    name: string;\n    response: object;\n  };\n}\n\nexport type GeminiPart =\n  | GeminiPartText\n  | GeminiPartInlineData\n  | GeminiPartFileData\n  | GeminiPartFunctionCall\n  | GeminiPartFunctionResponse;\n\nexport interface GeminiSafetySetting {\n  category: string;\n  threshold: string;\n}\n\nexport type GeminiSafetyRating = {\n  category: string;\n  probability: string;\n} & Record<string, unknown>;\n\nexport interface GeminiCitationMetadata {\n  citations: GeminiCitation[];\n}\n\nexport interface GeminiCitation {\n  startIndex: number;\n  endIndex: number;\n  uri: string;\n  title: string;\n  license: string;\n  publicationDate: GoogleTypeDate;\n}\n\nexport interface GoogleTypeDate {\n  year: number; // 1-9999 or 0 to specify a date without a year\n  month: number; // 1-12 or 0 to specify a year without a month and day\n  day: number; // Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant\n}\n\nexport interface GeminiGroundingMetadata {\n  webSearchQueries?: string[];\n  searchEntryPoint?: GeminiSearchEntryPoint;\n  groundingChunks: GeminiGroundingChunk[];\n  groundingSupports?: GeminiGroundingSupport[];\n  retrievalMetadata?: GeminiRetrievalMetadata;\n}\n\nexport interface GeminiSearchEntryPoint {\n  renderedContent?: string;\n  sdkBlob?: string; // Base64 encoded JSON representing array of tuple.\n}\n\nexport interface GeminiGroundingChunk {\n  web: GeminiGroundingChunkWeb;\n  retrievedContext: GeminiGroundingChunkRetrievedContext;\n}\n\nexport interface GeminiGroundingChunkWeb {\n  uri: string;\n  title: string;\n}\n\nexport interface GeminiGroundingChunkRetrievedContext {\n  uri: string;\n  title: string;\n  text: string;\n}\n\nexport interface GeminiGroundingSupport {\n  segment: GeminiSegment;\n  groundingChunkIndices: number[];\n  confidenceScores: number[];\n}\n\nexport interface GeminiSegment {\n  partIndex: number;\n  startIndex: number;\n  endIndex: number;\n  text: string;\n}\n\nexport interface GeminiRetrievalMetadata {\n  googleSearchDynamicRetrievalScore: number;\n}\n\nexport type GeminiUrlRetrievalStatus =\n  | \"URL_RETRIEVAL_STATUS_SUCCESS\"\n  | \"URL_RETRIEVAL_STATUS_ERROR\";\n\nexport interface GeminiUrlRetrievalContext {\n  retrievedUrl: string;\n  urlRetrievalStatus: GeminiUrlRetrievalStatus;\n}\n\nexport interface GeminiUrlRetrievalMetadata {\n  urlRetrievalContexts: GeminiUrlRetrievalContext[];\n}\n\nexport type GeminiUrlMetadata = GeminiUrlRetrievalContext;\n\nexport interface GeminiUrlContextMetadata {\n  urlMetadata: GeminiUrlMetadata[];\n}\n\nexport interface GeminiLogprobsResult {\n  topCandidates: GeminiLogprobsTopCandidate[];\n  chosenCandidates: GeminiLogprobsResultCandidate[];\n}\n\nexport interface GeminiLogprobsTopCandidate {\n  candidates: GeminiLogprobsResultCandidate[];\n}\n\nexport interface GeminiLogprobsResultCandidate {\n  token: string;\n  tokenId: number;\n  logProbability: number;\n}\n\n// The \"system\" content appears to only be valid in the systemInstruction\nexport type GeminiRole = \"system\" | \"user\" | \"model\" | \"function\";\n\nexport interface GeminiContent {\n  parts: GeminiPart[];\n  role: GeminiRole; // Vertex AI requires the role\n}\n\n/*\n * If additional attributes are added here, they should also be\n * added to the attributes below\n */\nexport interface GeminiTool {\n  functionDeclarations?: GeminiFunctionDeclaration[];\n  googleSearchRetrieval?: GoogleSearchRetrieval; // Gemini-1.5\n  googleSearch?: GoogleSearch; // Gemini-2.0\n  urlContext?: UrlContext;\n  retrieval?: VertexAIRetrieval;\n}\n\n/*\n * The known strings in this type should match those in GeminiSearchToolAttribuets\n */\nexport type GoogleSearchToolSetting =\n  | boolean\n  | \"googleSearchRetrieval\"\n  | \"googleSearch\"\n  | string;\n\nexport const GeminiSearchToolAttributes = [\n  \"googleSearchRetrieval\",\n  \"googleSearch\",\n];\n\nexport const GeminiToolAttributes = [\n  \"functionDeclaration\",\n  \"retrieval\",\n  \"urlContext\",\n  ...GeminiSearchToolAttributes,\n];\n\nexport interface GoogleSearchRetrieval {\n  dynamicRetrievalConfig?: {\n    mode?: string;\n    dynamicThreshold?: number;\n  };\n}\n\nexport interface GoogleSearch {}\n\nexport interface UrlContext {}\n\nexport interface VertexAIRetrieval {\n  vertexAiSearch: {\n    datastore: string;\n  };\n  disableAttribution?: boolean;\n}\n\nexport interface GeminiFunctionDeclaration {\n  name: string;\n  description: string;\n  parameters?: GeminiFunctionSchema;\n}\n\nexport interface GeminiFunctionSchema {\n  type: GeminiFunctionSchemaType;\n  format?: string;\n  description?: string;\n  nullable?: boolean;\n  enum?: string[];\n  properties?: Record<string, GeminiFunctionSchema>;\n  required?: string[];\n  items?: GeminiFunctionSchema;\n}\n\nexport type GeminiFunctionSchemaType =\n  | \"string\"\n  | \"number\"\n  | \"integer\"\n  | \"boolean\"\n  | \"array\"\n  | \"object\";\n\nexport interface GeminiGenerationConfig {\n  stopSequences?: string[];\n  candidateCount?: number;\n  maxOutputTokens?: number;\n  temperature?: number;\n  topP?: number;\n  topK?: number;\n  seed?: number;\n  presencePenalty?: number;\n  frequencyPenalty?: number;\n  responseMimeType?: GoogleAIResponseMimeType;\n  responseLogprobs?: boolean;\n  logprobs?: number;\n  responseModalities?: GoogleAIModelModality[];\n  thinkingConfig?: GoogleThinkingConfig;\n  speechConfig?: GoogleSpeechConfig;\n}\n\nexport interface GeminiRequest {\n  contents?: GeminiContent[];\n  systemInstruction?: GeminiContent;\n  tools?: GeminiTool[];\n  toolConfig?: {\n    functionCallingConfig: {\n      mode: \"auto\" | \"any\" | \"none\";\n      allowedFunctionNames?: string[];\n    };\n  };\n  safetySettings?: GeminiSafetySetting[];\n  generationConfig?: GeminiGenerationConfig;\n  cachedContent?: string;\n\n  /**\n   * Custom metadata labels to associate with the API call.\n   */\n  labels?: Record<string, string>;\n}\n\nexport interface GeminiResponseCandidate {\n  content: {\n    parts: GeminiPart[];\n    role: string;\n  };\n  finishReason: string;\n  index: number;\n  tokenCount?: number;\n  safetyRatings: GeminiSafetyRating[];\n  citationMetadata?: GeminiCitationMetadata;\n  groundingMetadata?: GeminiGroundingMetadata;\n  urlRetrievalMetadata?: GeminiUrlRetrievalMetadata;\n  urlContextMetadata?: GeminiUrlContextMetadata;\n  avgLogprobs?: number;\n  logprobsResult: GeminiLogprobsResult;\n  finishMessage?: string;\n}\n\ninterface GeminiResponsePromptFeedback {\n  blockReason?: string;\n  safetyRatings: GeminiSafetyRating[];\n}\n\nexport type ModalityEnum =\n  | \"TEXT\"\n  | \"IMAGE\"\n  | \"VIDEO\"\n  | \"AUDIO\"\n  | \"DOCUMENT\"\n  | string;\n\nexport interface ModalityTokenCount {\n  modality: ModalityEnum;\n  tokenCount: number;\n}\n\nexport interface GenerateContentResponseUsageMetadata {\n  promptTokenCount: number;\n  toolUsePromptTokenCount: number;\n  cachedContentTokenCount: number;\n  thoughtsTokenCount: number;\n  candidatesTokenCount: number;\n  totalTokenCount: number;\n\n  promptTokensDetails: ModalityTokenCount[];\n  toolUsePromptTokensDetails: ModalityTokenCount[];\n  cacheTokensDetails: ModalityTokenCount[];\n  candidatesTokensDetails: ModalityTokenCount[];\n\n  [key: string]: unknown;\n}\n\nexport interface GenerateContentResponseData {\n  candidates: GeminiResponseCandidate[];\n  promptFeedback: GeminiResponsePromptFeedback;\n  usageMetadata: GenerateContentResponseUsageMetadata;\n}\n\nexport type GoogleLLMModelFamily = null | \"palm\" | \"gemini\" | \"gemma\";\n\nexport type VertexModelFamily = GoogleLLMModelFamily | \"claude\";\n\nexport type GoogleLLMResponseData =\n  | JsonStream\n  | GenerateContentResponseData\n  | GenerateContentResponseData[];\n\nexport interface GoogleLLMResponse extends GoogleResponse {\n  data: GoogleLLMResponseData | AnthropicResponseData;\n}\n\nexport interface GoogleAISafetyHandler {\n  /**\n   * A function that will take a response and return the, possibly modified,\n   * response or throw an exception if there are safety issues.\n   *\n   * @throws GoogleAISafetyError\n   */\n  handle(response: GoogleLLMResponse): GoogleLLMResponse;\n}\n\nexport interface GoogleAISafetyParams {\n  safetyHandler?: GoogleAISafetyHandler;\n}\n\nexport type GeminiJsonSchema = Record<string, unknown> & {\n  properties?: Record<string, GeminiJsonSchema>;\n  type: GeminiFunctionSchemaType;\n  nullable?: boolean;\n};\n\nexport interface GeminiJsonSchemaDirty extends GeminiJsonSchema {\n  items?: GeminiJsonSchemaDirty;\n  properties?: Record<string, GeminiJsonSchemaDirty>;\n  additionalProperties?: boolean;\n}\n\nexport type GoogleAIAPI = {\n  messageContentToParts?: (content: MessageContent) => Promise<GeminiPart[]>;\n\n  baseMessageToContent?: (\n    message: BaseMessage,\n    prevMessage: BaseMessage | undefined,\n    useSystemInstruction: boolean\n  ) => Promise<GeminiContent[]>;\n\n  responseToString: (response: GoogleLLMResponse) => string;\n\n  responseToChatGeneration: (\n    response: GoogleLLMResponse\n  ) => ChatGenerationChunk | null;\n\n  chunkToString: (chunk: BaseMessageChunk) => string;\n\n  responseToBaseMessage: (response: GoogleLLMResponse) => BaseMessage;\n\n  responseToChatResult: (response: GoogleLLMResponse) => ChatResult;\n\n  formatData: (\n    input: unknown,\n    parameters: GoogleAIModelRequestParams\n  ) => Promise<unknown>;\n};\n\nexport interface GeminiAPIConfig {\n  safetyHandler?: GoogleAISafetyHandler;\n  mediaManager?: MediaManager;\n  useSystemInstruction?: boolean;\n\n  /**\n   * How to handle the Google Search tool, since the name (and format)\n   * of the tool changes between Gemini 1.5 and Gemini 2.0.\n   * true - Change based on the model version. (Default)\n   * false - Do not change the tool name provided\n   * string value - Use this as the attribute name for the search\n   *   tool, adapting any tool attributes if possible.\n   * When the model is created, a \"true\" or default setting\n   * will be changed to a string based on the model.\n   */\n  googleSearchToolAdjustment?: GoogleSearchToolSetting;\n}\n\nexport type GoogleAIAPIConfig = GeminiAPIConfig | AnthropicAPIConfig;\n\nexport interface GoogleAIAPIParams {\n  apiName?: string;\n  apiConfig?: GoogleAIAPIConfig;\n}\n\n// Embeddings\n\n/**\n * Defines the parameters required to initialize a\n * GoogleEmbeddings instance. It extends EmbeddingsParams and\n * GoogleConnectionParams.\n */\nexport interface BaseGoogleEmbeddingsParams<AuthOptions>\n  extends EmbeddingsParams,\n    GoogleConnectionParams<AuthOptions> {\n  model: string;\n\n  /**\n   * Used to specify output embedding size.\n   * If set, output embeddings will be truncated to the size specified.\n   */\n  dimensions?: number;\n\n  /**\n   * An alias for \"dimensions\"\n   */\n  outputDimensionality?: number;\n}\n\n/**\n * Defines additional options specific to the\n * GoogleEmbeddingsInstance. It extends AsyncCallerCallOptions.\n */\nexport interface BaseGoogleEmbeddingsOptions extends AsyncCallerCallOptions {}\n\nexport type GoogleEmbeddingsTaskType =\n  | \"RETRIEVAL_QUERY\"\n  | \"RETRIEVAL_DOCUMENT\"\n  | \"SEMANTIC_SIMILARITY\"\n  | \"CLASSIFICATION\"\n  | \"CLUSTERING\"\n  | \"QUESTION_ANSWERING\"\n  | \"FACT_VERIFICATION\"\n  | \"CODE_RETRIEVAL_QUERY\"\n  | string;\n\n/**\n * Represents an instance for generating embeddings using the Google\n * Vertex AI API. It contains the content to be embedded.\n */\nexport interface VertexEmbeddingsInstance {\n  content: string;\n  taskType?: GoogleEmbeddingsTaskType;\n  title?: string;\n}\n\nexport interface VertexEmbeddingsParameters extends GoogleModelParams {\n  autoTruncate?: boolean;\n  outputDimensionality?: number;\n}\n\nexport interface VertexEmbeddingsRequest {\n  instances: VertexEmbeddingsInstance[];\n  parameters?: VertexEmbeddingsParameters;\n}\n\nexport interface AIStudioEmbeddingsRequest {\n  content: {\n    parts: GeminiPartText[];\n  };\n  model?: string; // Documentation says required, but tests say otherwise\n  taskType?: GoogleEmbeddingsTaskType;\n  title?: string;\n  outputDimensionality?: number;\n}\n\nexport type GoogleEmbeddingsRequest =\n  | VertexEmbeddingsRequest\n  | AIStudioEmbeddingsRequest;\n\nexport interface VertexEmbeddingsResponsePrediction {\n  embeddings: {\n    statistics: {\n      token_count: number;\n      truncated: boolean;\n    };\n    values: number[];\n  };\n}\n\n/**\n * Defines the structure of the embeddings results returned by the Google\n * Vertex AI API. It extends GoogleBasePrediction and contains the\n * embeddings and their statistics.\n */\nexport interface VertexEmbeddingsResponse extends GoogleResponse {\n  data: {\n    predictions: VertexEmbeddingsResponsePrediction[];\n  };\n}\n\nexport interface AIStudioEmbeddingsResponse extends GoogleResponse {\n  data: {\n    embedding: {\n      values: number[];\n    };\n  };\n}\n\nexport type GoogleEmbeddingsResponse =\n  | VertexEmbeddingsResponse\n  | AIStudioEmbeddingsResponse;\n"],"mappings":";;AAmEA,MAAa,yBAAyB;CACpC,YAAY;CACZ,YAAY;CACZ,0BAA0B;CAE1B,YAAY;CACZ,aAAa;CACb,2BAA2B;CAE3B,kBAAkB;CAClB,mBAAmB;CACnB,iCAAiC;CAEjC,WAAW;CACX,WAAW;CACX,yBAAyB;CAEzB,gBAAgB;CAChB,iBAAiB;CACjB,+BAA+B;AAChC;AAKD,MAAa,0BAA0B;CACrC,MAAM;CACN,MAAM;CACN,YAAY;CAEZ,KAAK;CACL,KAAK;CACL,iBAAiB;CAEjB,MAAM;CACN,MAAM;CACN,wBAAwB;CAExB,MAAM;CACN,MAAM;CACN,qBAAqB;CAErB,KAAK;CACL,KAAK;CACL,WAAW;AACZ;AAKD,MAAa,uBAAuB;CAClC,UAAU;CACV,aAAa;AACd;AA4gBD,MAAa,6BAA6B,CACxC,yBACA,cACD;AAED,MAAa,uBAAuB;CAClC;CACA;CACA;CACA,GAAG;AACJ"}